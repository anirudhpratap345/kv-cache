FULL EXTRACTED PDF CONTENT
================================================================================


--- PAGE 1 ---
QLORA: Efficient Finetuning of Quantized LLMs
TimDettmers∗ ArtidoroPagnoni∗ AriHoltzman
LukeZettlemoyer
UniversityofWashington
{dettmers,artidoro,ahai,lsz}@cs.washington.edu
Abstract
We present QLORA, an efficient finetuning approach that reduces memory us-
age enough to finetune a 65B parameter model on a single 48GB GPU while
preservingfull16-bitfinetuningtaskperformance. QLORAbackpropagatesgradi-
entsthroughafrozen,4-bitquantizedpretrainedlanguagemodelintoLowRank
Adapters(LoRA).Ourbestmodelfamily,whichwenameGuanaco,outperforms
allpreviousopenlyreleasedmodelsontheVicunabenchmark, reaching99.3%
oftheperformancelevelofChatGPTwhileonlyrequiring24hoursoffinetuning
onasingleGPU. QLORA introducesanumberofinnovationstosavememory
withoutsacrificingperformance: (a)4-bitNormalFloat(NF4),anewdatatypethat
isinformationtheoreticallyoptimalfornormallydistributedweights(b)Double
Quantizationtoreducetheaveragememoryfootprintbyquantizingthequantization
constants,and(c)PagedOptimizerstomanagememoryspikes. WeuseQLORA
tofinetunemorethan1,000models,providingadetailedanalysisofinstruction
followingandchatbotperformanceacross8instructiondatasets,multiplemodel
types(LLaMA,T5),andmodelscalesthatwouldbeinfeasibletorunwithregular
finetuning(e.g. 33Band65Bparametermodels). OurresultsshowthatQLoRA
finetuning on a small high-quality dataset leads to state-of-the-art results, even
whenusingsmallermodelsthanthepreviousSoTA.Weprovideadetailedanalysis
ofchatbotperformancebasedonbothhumanandGPT-4evaluationsshowingthat
GPT-4evaluationsareacheapandreasonablealternativetohumanevaluation. Fur-
thermore,wefindthatcurrentchatbotbenchmarksarenottrustworthytoaccurately
evaluatetheperformancelevelsofchatbots. Alemon-pickedanalysisdemonstrates
whereGuanacofailscomparedtoChatGPT.Wereleaseallofourmodelsandcode,
includingCUDAkernelsfor4-bittraining.2
1 Introduction
Finetuninglargelanguagemodels(LLMs)isahighlyeffectivewaytoimprovetheirperformance,
[40,62,43,61,59,37]andtoadddesirableorremoveundesirablebehaviors[43,2,4]. However,
finetuningverylargemodelsisprohibitivelyexpensive;regular16-bitfinetuningofaLLaMA65B
parameter model [57] requires more than 780 GB of GPU memory. While recent quantization
methodscanreducethememoryfootprintofLLMs[14,13,18,66],suchtechniquesonlyworkfor
inferenceandbreakdownduringtraining[65].
Wedemonstrateforthefirsttimethatitispossibletofinetuneaquantized4-bitmodelwithoutany
performancedegradation. Ourmethod,QLORA,usesanovelhigh-precisiontechniquetoquantize
a pretrained model to 4-bit, then adds a small set of learnable Low-rank Adapter weights [28]
∗Equalcontribution.
2https://github.com/artidoro/qloraandhttps://github.com/TimDettmers/bitsandbytes
Preprint.Underreview.
3202
yaM
32
]GL.sc[
1v41341.5032:viXra
--- PAGE 2 ---
Table 1: Elo ratings for a competition between
thataretunedbybackpropagatinggradientsthrough
models,averagedfor10,000randominitialorder-
thequantizedweights.
ings. The winner of a match is determined by
GPT-4whichdeclareswhichresponseisbetterfor
QLORAreducestheaveragememoryrequirements
agivenpromptofthetheVicunabenchmark.95%
offinetuninga65Bparametermodelfrom>780GB
confidenceintervalsareshown(±). AfterGPT-
ofGPUmemoryto<48GBwithoutdegradingthe
4,Guanaco33Band65Bwinthemostmatches,
runtimeorpredictiveperformancecomparedtoa16-
whileGuanaco13BscoresbetterthanBard.
bitfullyfinetunedbaseline. Thismarksasignificant
shift in accessibility of LLM finetuning: now the Model Size Elo
largestpubliclyavailablemodelstodatefinetunable
GPT-4 - 1348±1
onasingleGPU.UsingQLORA,wetraintheGua-
Guanaco65B 41GB 1022±1
nacofamilyofmodels,withthesecondbestmodel
Guanaco33B 21GB 992±1
reaching97.8%oftheperformancelevelofChatGPT
Vicuna13B 26GB 974±1
ontheVicuna[10]benchmark,whilebeingtrainable
ChatGPT - 966±1
in less than 12 hours on a single consumer GPU;
Guanaco13B 10GB 916±1
using a single professional GPU over 24 hours we
Bard - 902±1
achieve 99.3% with our largest model, essentially
Guanaco7B 6GB 879±1
closing the gap to ChatGPT on the Vicuna bench-
mark. Whendeployed,oursmallestGuanacomodel
(7Bparameters)requiresjust5GBofmemoryandoutperformsa26GBAlpacamodelbymorethan
20percentagepointsontheVicunabenchmark(Table6).
QLORA introducesmultipleinnovationsdesignedtoreducememoryusewithoutsacrificingper-
formance: (1)4-bitNormalFloat,aninformationtheoreticallyoptimalquantizationdatatypefor
normally distributed data that yields better empirical results than 4-bit Integers and 4-bit Floats.
(2)DoubleQuantization, amethodthatquantizesthequantizationconstants, savinganaverage
ofabout0.37bitsperparameter(approximately3GBfora65Bmodel). (3)PagedOptimizers,
usingNVIDIAunifiedmemorytoavoidthegradientcheckpointingmemoryspikesthatoccurwhen
processingamini-batchwithalongsequencelength. Wecombinethesecontributionsintoabetter
tunedLoRAapproachthatincludesadaptersateverynetworklayerandtherebyavoidsalmostallof
theaccuracytradeoffsseeninpriorwork.
QLORA’sefficiencyenablesustoperformanin-depthstudyofinstructionfinetuningandchatbot
performance on model scales that would be impossible using regular finetuning due to memory
overhead. Therefore,wetrainmorethan1,000modelsacrossseveralinstructiontuningdatasets,
modelarchitectures,andsizesbetween80Mto65Bparameters. InadditiontoshowingthatQLORA
recovers 16-bit performance (§4) and training a state-of-the-art chatbot, Guanaco, (§5), we also
analyze trends in the trained models. First, we find that data quality is far more important than
datasetsize,e.g.,a9ksampledataset(OASST1)outperformeda450ksampledataset(FLANv2,
subsampled)onchatbotperformance,evenwhenbotharemeanttosupportinstructionfollowing
generalization. Second,weshowthatstrongMassiveMultitaskLanguageUnderstanding(MMLU)
benchmarkperformancedoesnotimplystrongVicunachatbotbenchmarkperformanceandvice
versa—inotherwords,datasetsuitabilitymattersmorethansizeforagiventask.
Furthermore, we also provide a extensive analysis of chatbot performance that uses both human
ratersandGPT-4forevaluation. Weusetournament-stylebenchmarkingwheremodelscompete
against each other in matches to produce the best response for a given prompt. The winner of a
matchisjudgedbyeitherGPT-4orhumanannotators. Thetournamentresultsareaggregatedinto
Eloscores[16,17]whichdeterminetherankingofchatbotperformance. WefindthatGPT-4and
humanevaluationslargelyagreeontherankofmodelperformanceinthetournaments,butwealso
findthereareinstancesofstrongdisagreement. Assuch,wehighlightthatmodel-basedevaluation
whileprovidingacheapalternativetohuman-annotationalsohasitsuncertainties.
WeaugmentourchatbotbenchmarkresultswithaqualitativeanalysisofGuanacomodels.Ouranaly-
sishighlightssuccessandfailurecasesthatwerenotcapturedbythequantitativebenchmarks.
WereleaseallmodelgenerationswithhumanandGPT-4annotationstofacilitatefurtherstudy. We
open-source our codebase and CUDA kernels and integrate our methods into the Hugging Face
transformersstack[64],makingthemeasilyaccessibletoall. Wereleaseacollectionofadapters
for7/13/33/65Bsizemodels,trainedon8differentinstructionfollowingdatasets,foratotalof32
differentopensourced,finetunedmodels.
2
--- PAGE 3 ---
Figure1: Differentfinetuningmethodsandtheirmemoryrequirements. QLORAimprovesoverLoRAby
quantizingthetransformermodelto4-bitprecisionandusingpagedoptimizerstohandlememoryspikes.
2 Background
Block-wisek-bitQuantization Quantizationistheprocessofdiscretizinganinputfromarep-
resentationthatholdsmoreinformationtoarepresentationwithlessinformation. Itoftenmeans
takingadatatypewithmorebitsandconvertingittofewerbits,forexamplefrom32-bitfloatsto
8-bitIntegers. Toensurethattheentirerangeofthelow-bitdatatypeisused,theinputdatatypeis
commonlyrescaledintothetargetdatatyperangethroughnormalizationbytheabsolutemaximum
oftheinputelements, whichareusuallystructuredasatensor. Forexample, quantizinga32-bit
FloatingPoint(FP32)tensorintoaInt8tensorwithrange[−127,127]:
(cid:18) (cid:19)
127
XInt8 =round XFP32 =round(cFP32·XFP32), (1)
absmax(XFP32)
wherecisthequantizationconstantorquantizationscale. Dequantizationistheinverse:
XInt8
dequant(cFP32,XInt8)= =XFP32 (2)
cFP32
Theproblemwiththisapproachisthatifalargemagnitudevalue(i.e.,anoutlier)occursintheinput
tensor,thenthequantizationbins—certainbitcombinations—arenotutilizedwellwithfeworno
numbersquantizedinsomebins. Topreventtheoutlierissue,acommonapproachistochunkthe
inputtensorintoblocksthatareindependentlyquantized,eachwiththeirownquantizationconstantc.
Thiscanbeformalizedasfollows: WechunktheinputtensorX∈Rb×hintoncontiguousblocksof
sizeBbyflatteningtheinputtensorandslicingthelinearsegmentinton=(b×h)/Bblocks. We
quantizetheseblocksindependentlywithEquation1tocreateaquantizedtensorandnquantization
constantsc .
i
Low-rankAdapters Low-rankAdapter(LoRA)finetuning[28]isamethodthatreducesmemory
requirementsbyusingasmallsetoftrainableparameters,oftentermedadapters,whilenotupdating
the full model parameters which remain fixed. Gradients during stochastic gradient descent are
passedthroughthefixedpretrainedmodelweightstotheadapter,whichisupdatedtooptimizethe
lossfunction. LoRAaugmentsalinearprojectionthroughanadditionalfactorizedprojection. Given
aprojectionXW=YwithX∈Rb×h,W∈Rh×oLoRAcomputes:
Y =XW+sXL L , (3)
1 2
whereL ∈Rh×r andL ∈Rr×o,andsisascalar.
1 2
MemoryRequirementofParameter-EfficientFinetuning Oneimportantpointofdiscussionis
thememoryrequirementofLoRAduringtrainingbothintermsofthenumberandsizeofadapters
used. SincethememoryfootprintofLoRAissominimal, wecanusemoreadapterstoimprove
performancewithoutsignificantlyincreasingthetotalmemoryused. WhileLoRAwasdesignedasa
3
--- PAGE 4 ---
ParameterEfficientFinetuning(PEFT)method,mostofthememoryfootprintforLLMfinetuning
comes from activation gradients and not from the learned LoRA parameters. For a 7B LLaMA
modeltrainedonFLANv2withabatchsizeof1,withLoRAweightsequivalenttocommonlyused
0.2% of the original model weights[28, 37], the LoRA input gradients have a memory footprint
of567MBwhiletheLoRAparameterstakeuponly26MB.Withgradientcheckpointing[9],the
inputgradientsreducetoanaverageof18MBpersequencemakingthemmorememoryintensive
than all LoRA weights combined. In comparison, the 4-bit base model consumes 5,048 MB of
memory. Thishighlightsthatgradientcheckpointingisimportantbutalsothataggressivelyreducing
theamountofLoRAparameteryieldsonlyminormemorybenefits. Thismeanswecanusemore
adapters without significantly increasing the overall training memory footprint (see Appendix G
for a detailed breakdown). As discussed later, this is crucial for recovering full 16-bit precision
performance.
3 QLORA Finetuning
QLORAachieveshigh-fidelity4-bitfinetuningviatwotechniqueswepropose—4-bitNormalFloat
(NF4) quantization and Double Quantization. Additionally, we introduce Paged Optimizers, to
preventmemoryspikesduringgradientcheckpointingfromcausingout-of-memoryerrorsthathave
traditionallymadefinetuningonasinglemachinedifficultforlargemodels.
QLORAhasonelow-precisionstoragedatatype,inourcaseusually4-bit,andonecomputationdata
typethatisusuallyBFloat16. Inpractice,thismeanswheneveraQLORAweighttensorisused,we
dequantizethetensortoBFloat16,andthenperformamatrixmultiplicationin16-bit.
WenowdiscussthecomponentsofQLORAfollowedbyaformaldefinitionofQLORA.
4-bitNormalFloatQuantization TheNormalFloat(NF)datatypebuildsonQuantileQuantization
[15]whichisaninformation-theoreticallyoptimaldatatypethatensureseachquantizationbinhasan
equalnumberofvaluesassignedfromtheinputtensor. Quantilequantizationworksbyestimating
thequantileoftheinputtensorthroughtheempiricalcumulativedistributionfunction.
Themainlimitationofquantilequantizationisthattheprocessofquantileestimationisexpensive.
Thereforefastquantileapproximationalgorithms,suchasSRAMquantiles[15],areusedtoestimate
them. Duetotheapproximatenatureofthesequantileestimationalgorithms,thedatatypehaslarge
quantizationerrorsforoutliers,whichareoftenthemostimportantvalues.
Expensivequantileestimatesandapproximationerrorscanbeavoidedwheninputtensorscomefrom
adistributionfixeduptoaquantizationconstant. Insuchcases,inputtensorshavethesamequantiles
makingexactquantileestimationcomputationallyfeasible.
Since pretrained neural network weights usually have a zero-centered normal distribution with
standarddeviationσ(seeAppendixF),wecantransformallweightstoasinglefixeddistributionby
scalingσsuchthatthedistributionfitsexactlyintotherangeofourdatatype. Forourdatatype,we
setthearbitraryrange[−1,1]. Assuch,boththequantilesforthedatatypeandtheneuralnetwork
weightsneedtobenormalizedintothisrange.
Theinformationtheoreticallyoptimaldatatypeforzero-meannormaldistributionswitharbitrary
standarddeviationsσintherange[−1,1]iscomputedasfollows: (1)estimatethe2k+1quantiles
ofatheoreticalN(0,1)distributiontoobtainak-bitquantilequantizationdatatypefornormaldistri-
butions,(2)takethisdatatypeandnormalizeitsvaluesintothe[−1,1]range,(3)quantizeaninput
weighttensorbynormalizingitintothe[−1,1]rangethroughabsolutemaximumrescaling.
Oncetheweightrangeanddatatyperangematch,wecanquantizeasusual. Step(3)isequivalentto
rescalingthestandarddeviationoftheweighttensortomatchthestandarddeviationofthek-bitdata
type. Moreformally,weestimatethe2k valuesq ofthedatatypeasfollows:
i
(cid:18) (cid:18) (cid:19) (cid:18) (cid:19)(cid:19)
1 i i+1
q = Q +Q , (4)
i 2 X 2k+1 X 2k+1
whereQ (·)isthequantilefunctionofthestandardnormaldistributionN(0,1). Aproblemfor
X
asymmetrick-bitquantizationisthatthisapproachdoesnothaveanexactrepresentationofzero,
whichisanimportantpropertytoquantizepaddingandotherzero-valuedelementswithnoerror. To
4
--- PAGE 5 ---
ensureadiscretezeropointof0andtouseall2k bitsforak-bitdatatype,wecreateanasymmetric
datatypebyestimatingthequantilesq oftworangesq : 2k−1forthenegativepartand2k−1+1for
i i
thepositivepartandthenweunifythesesetsofq andremoveoneofthetwozerosthatoccursinboth
i
sets.Wetermtheresultingdatatypethathasequalexpectednumberofvaluesineachquantizationbin
k-bitNormalFloat(NFk),sincethedatatypeisinformation-theoreticallyoptimalforzero-centered
normallydistributeddata. TheexactvaluesofthisdatatypecanbefoundinAppendixE.
Double Quantization We introduce Double Quantization (DQ), the process of quantizing the
quantizationconstantsforadditionalmemorysavings. Whileasmallblocksizeisrequiredforprecise
4-bit quantization [13], it also has a considerable memory overhead. For example, using 32-bit
constantsandablocksizeof64forW,quantizationconstantsadd32/64=0.5bitsperparameteron
average. DoubleQuantizationhelpsreducethememoryfootprintofquantizationconstants.
Morespecifically,DoubleQuantizationtreatsquantizationconstantscFP32 ofthefirstquantization
2
as inputs to a second quantization. This second step yields the quantized quantization constants
cFP8 andthesecondlevelofquantizationconstantscFP32. Weuse8-bitFloatswithablocksizeof
2 1
256forthesecondquantizationasnoperformancedegradationisobservedfor8-bitquantization,
inlinewithresultsfromDettmersandZettlemoyer[13]. SincethecFP32 arepositive,wesubtract
2
themeanfromc beforequantizationtocenterthevaluesaroundzeroandmakeuseofsymmetric
2
quantization. Onaverage,forablocksizeof64,thisquantizationreducesthememoryfootprintper
parameterfrom32/64 = 0.5bits,to8/64+32/(64·256) = 0.127bits,areductionof0.373bits
perparameter.
PagedOptimizers usetheNVIDIAunifiedmemory3featurewichdoesautomaticpage-to-page
transfersbetweentheCPUandGPUforerror-freeGPUprocessinginthescenariowheretheGPU
occasionallyrunsout-of-memory. ThefeatureworkslikeregularmemorypagingbetweenCPURAM
andthedisk. Weusethisfeaturetoallocatepagedmemoryfortheoptimizerstateswhicharethen
automaticallyevictedtoCPURAMwhentheGPUrunsout-of-memoryandpagedbackintoGPU
memorywhenthememoryisneededintheoptimizerupdatestep.
QLORA. Usingthecomponentsdescribedabove,wedefineQLORAforasinglelinearlayerin
thequantizedbasemodelwithasingleLoRAadapterasfollows:
YBF16 =XBF16doubleDequant(cFP32,ck-bit,WNF4)+XBF16LBF16LBF16, (5)
1 2 1 2
wheredoubleDequant(·)isdefinedas:
doubleDequant(cFP32,ck-bit,Wk-bit)=dequant(dequant(cFP32,ck-bit),W4bit)=WBF16, (6)
1 2 1 2
WeuseNF4forWandFP8forc . Weuseablocksizeof64forWforhigherquantizationprecision
2
andablocksizeof256forc toconservememory.
2
Forparameterupdatesonlythegradientwithrespecttotheerrorfortheadaptersweights ∂E are
∂Li
needed,andnotfor4-bitweights ∂E . However,thecalculationof ∂E entailsthecalculationof ∂X
∂W ∂Li ∂W
whichproceedsviaequation(5)withdequantizationfromstorageWNF4tocomputationdatatype
WBF16tocalculatethederivative ∂X inBFloat16precision.
∂W
Tosummarize,QLORAhasonestoragedatatype(usually4-bitNormalFloat)andacomputation
data type (16-bit BrainFloat). We dequantize the storage data type to the computation data type
toperformtheforwardandbackwardpass, butweonlycomputeweightgradientsfortheLoRA
parameterswhichuse16-bitBrainFloat.
4 QLoRAvs. StandardFinetuning
WehavediscussedhowQLoRAworksandhowitcansignificantlyreducetherequiredmemoryfor
finetuningmodels. ThemainquestionnowiswhetherQLoRAcanperformaswellasfull-model
finetuning. Furthermore,wewanttoanalyzethecomponentsofQLoRAincludingtheimpactof
NormalFloat4overstandardFloat4. Thefollowingsectionswilldiscusstheexperimentsthataimed
atansweringthesequestions.
3
https://docs.nvidia.com/cuda/cuda-c-programming-guide
5
--- PAGE 6 ---
Experimentalsetup. Weconsiderthreearchitectures(encoder,encoder-decoder,anddecoderonly)
andcompareQLoRAwith16-bitadapter-finetuningandwithfull-finetuningformodelsupto3B.Our
evaluationsincludeGLUE[58]withRoBERTa-large[38],Super-NaturalInstructions(TKInstruct)
[61] with T5 [49], and 5-shot MMLU [24] after finetuning LLaMA on Flan v2 [39] and Alpaca
[55]. ToadditionallystudytheadvantagesofNF4overother4-bitdatatypes,weusethesetupof
DettmersandZettlemoyer[13]andmeasurepost-quantizationzero-shotaccuracyandperplexity
acrossdifferentmodels(OPT[72],LLaMA[57],BLOOM[52],Pythia[7])formodelsizes125m-
13B.Weprovidemoredetailsintheresultssectionforeachparticularsetuptomaketheresultsmore
readable. FulldetailsinAppendixA.
64
63
62
61
60
QLoRA-All QLoRA-FFN QLoRA-Attention
Alpaca
(ours) Stanford-Alpaca
Model
LeguoR
Whilepagedoptimizersarecriticaltodo33B/65B
QLORAtuningonasingle24/48GBGPU,wedo
notprovidehardmeasurementsforPagedOptimiz-
erssincethepagingonlyoccurswhenprocessing
mini-batcheswithlongsequencelengths,whichis
rare. Wedo, however, performananalysisofthe
runtime of paged optimizers for 65B models on
48GBGPUsandfindthatwithabatchsizeof16, bits
4
pagedoptimizersprovidethesametrainingspeed 16
asregularoptimizers. Futureworkshouldmeasure
and characterize under what circumstances slow-
downsoccurfromthepagingprocess.
DefaultLoRAhyperparametersdonotmatch16-
bitperformance Whenusingthestandardprac-
ticeofapplyingLoRAtoqueryandvalueattention
projectionmatrices[28],wearenotabletoreplicate Figure2: RougeLforLLaMA7Bmodelsonthe
fullfinetuningperformanceforlargebasemodels. Alpacadataset. Eachpointrepresentsarunwitha
AsshowninFigure2forLLaMA7Bfinetuningon differentrandomseed.WeimproveontheStanford
Alpaca,wefindthatthemostcriticalLoRAhyper- Alpacafullyfinetuneddefaulthyperparametersto
constructastrong16-bitbaselineforcomparisons.
parameterishowmanyLoRAadaptersareusedin
UsingLoRAonalltransformerlayersiscriticalto
totalandthatLoRAonalllineartransformerblock
match16-bitperformance.
layersarerequiredtomatchfullfinetuningperfor-
mance. OtherLoRAhyperparameters,suchasthe
projectiondimensionr,donotaffectperformance(seeAppendixA).
0.67
0.66
0.65
0.64
0.63
0.62
0.61
0.60
1010 1011
Total model bits
ycarucca
tohsorez
naeM
Similarly,wefindthatdefaulthyperparametersfor
fullyfinetunedbaselinesareundertuned. Wedoa
4-bit LLaMA
hyperparametersearchoverlearningrates1e-6to
5e-5andbatchsizes8to128tofindrobustbaselines.
Results for 7B LLaMA finetuning on Alpaca are
showninFigure2.
4-bit NormalFloat yields better performance
than 4-bit Floating Point While the 4-bit
NormalFloat (NF4) data type is information-
Data type
theoreticallyoptimal,itstillneedstobedetermined Float
NFloat
ifthispropertytranslatestoempiricaladvantages.
NFloat + DQ
WefollowthesetupfromDettmersandZettlemoyer
[13]wherequantizedLLMs(OPT[72],BLOOM
[52],Pythia[7],LLaMA)ofdifferentsizes(125M Figure 3: Mean zero-shot accuracy over Wino-
to65B)withdifferentdatatypesareevaluatedon grande, HellaSwag, PiQA, Arc-Easy, and Arc-
languagemodelingandasetofzero-shottasks. In ChallengeusingLLaMAmodelswithdifferent4-bit
Figure3andTable2weseethatNF4improvesper- datatypes.TheNormalFloatdatatypesignificantly
formancesignificantlyoverFP4andInt4andthat improves the bit-for-bit accuracy gains compared
doublequantizationreducesthememoryfootprint toregular4-bitFloats.WhileDoubleQuantization
withoutdegradingperformance. (DQ)onlyleadstominorgains,itallowsforamore
fine-grainedcontroloverthememoryfootprinttofit
k-bitQLORAmatches16-bitfullfinetuningand modelsofcertainsize(33B/65B)intocertainGPUs
16-bitLoRAperformance Recentfindingshave (24/48GB).
establishedthat4-bitquantizationforinferenceis
6
--- PAGE 7 ---
Table 3: Experiments comparing 16-bit BrainFloat (BF16), 8-bit Integer (Int8), 4-bit Float (FP4), and 4-
bitNormalFloat(NF4)onGLUEandSuper-NaturalInstructions. QLORAreplicates16-bitLoRAandfull-
finetuning.
Dataset GLUE(Acc.) Super-NaturalInstructions(RougeL)
Model RoBERTa-large T5-80M T5-250M T5-780M T5-3B T5-11B
BF16 88.6 40.1 42.1 48.0 54.3 62.0
BF16replication 88.6 40.0 42.2 47.3 54.9 -
LoRABF16 88.8 40.5 42.6 47.1 55.4 60.7
QLORAInt8 88.8 40.4 42.9 45.4 56.5 60.7
QLORAFP4 88.6 40.3 42.4 47.5 55.6 60.9
QLORANF4+DQ - 40.4 42.7 47.7 55.3 60.9
possible,butleadstoperformancedegradationrel-
ative to 16-bit [13, 18]. This raises the crucial question of whether the lost performance can be
recoveredbyconducting4-bitadapterfinetuning. Wetestthisfortwosetups.
The first focuses on a comparison with full 16-bit finetuning
ofRoBERTAandT5modelssized125Mto3Bparameterson Table2:PileCommonCrawlmean
GLUE and the Super-NaturalInstructions dataset. Results are perplexity for different data types
showninTable3. Inbothdatasets,weobservethat16-bit,8-bit, for 125M to 13B OPT, BLOOM,
and4-bitadaptermethodsreplicatetheperformanceofthefully LLaMA,andPythiamodels.
finetuned16-bitbaseline. Thissuggeststhattheperformancelost
duetotheimprecisequantizationcanbefullyrecoveredthrough Datatype MeanPPL
adapterfinetuningafterquantization.
Int4 34.34
Float4(E2M1) 31.07
Foroursecondsetup,sincefullfinetuningmodelsatandbeyond
Float4(E3M0) 29.48
11Bparametersrequiresmorethanoneserverofhighmemory
NFloat4+DQ 27.41
GPUs, we continue to test whether 4-bit QLORA can match
16-bitLoRAatthe7Bto65Bparameterscales. Tothisend,we
finetuneLLaMA7Bthrough65Bontwoinstructionfollowing
datasets,AlpacaandFLANv2,andevaluateontheMMLUbenchmarkvia5-shotaccuracy. Results
are shown in Table 4 where we see that NF4 with double quantization fully recovers the 16-bit
LoRAMMLUperformance. Inaddition,wealsonotethatQLORAwithFP4lagsbehindthe16-bit
brainfloatLoRAbaselinebyabout1percentagepoint. Thiscorroboratesbothourfindingsthat(1)
QLORAwithNF4replicatesboth16-bitfullfinetuningand16-bitLoRAfinetuningperformance,
and(2)NF4issuperiortoFP4intermsofquantizationprecision.
Summary Our results consistently show that 4-bit QLORA with NF4 data type matches 16-
bit full finetuning and 16-bit LoRA finetuning performance on academic benchmarks with well-
establishedevaluationsetups. Wehavealsoshownthat NF4ismoreeffectivethanFP4andthat
doublequantizationdoesnotdegradeperformance. Combined,thisformscompellingevidencethat
4-bitQLORAtuningreliablyyieldsresultsmatching16-bitmethods.
Inlinewithpreviousworkonquantization[13],ourMMLUandEloresultsindicatethatwithagiven
finetuningandinferenceresourcebudgetitisbeneficialtoincreasethenumberofparametersinthe
basemodelwhiledecreasingtheirprecision. Thishighlightstheimportanceofefficiencybenefits
fromQLORA.Sincewedidnotobserveperformancedegradationcomparedtofull-finetuningin
ourexperimentswith4-bitfinetuning,thisraisesthequestionofwheretheperformance-precision
trade-offexactlyliesforQLoRAtuning,whichweleavetofutureworktoexplore.
Weproceedtoinvestigateinstructiontuningatscalesthatwouldbeimpossibletoexplorewithfull
16-bitfinetuningonacademicresearchhardware.
5 PushingtheChatbotState-of-the-artwithQLoRA
Havingestablishedthat4-bitQLORAmatches16-bitperformanceacrossscales,tasks,anddatasets
weconductanin-depthstudyofinstructionfinetuninguptothelargestopen-sourcelanguagemodels
availableforresearch. Toassesstheperformanceofinstructionfinetuningthesemodels,weevaluate
7
--- PAGE 8 ---
Table4:Mean5-shotMMLUtestaccuracyforLLaMA7-65BmodelsfinetunedwithadaptersonAlpacaand
FLANv2fordifferentdatatypes.Overall,NF4withdoublequantization(DQ)matchesBFloat16performance,
whileFP4isconsistentlyonepercentagepointbehindboth.
Mean5-shotMMLUAccuracy
LLaMASize 7B 13B 33B 65B Mean
Dataset Alpaca FLANv2 Alpaca FLANv2 Alpaca FLANv2 Alpaca FLANv2
BFloat16 38.4 45.6 47.2 50.6 57.7 60.5 61.8 62.5 53.0
Float4 37.2 44.0 47.3 50.0 55.9 58.5 61.3 63.3 52.2
NFloat4+DQ 39.0 44.5 47.5 50.7 57.3 59.2 61.8 63.9 53.1
onachallengingNaturalLanguageUnderstandingbenchmark(MMLU)anddevelopnewmethods
forreal-worldchatbotperformanceevaluation.
5.1 Experimentalsetup
WenowdescribeanoverviewoftheexperimentalsetupwithfulldetailsinAppendixB.
Data As,toourknowledge,thereisnocomprehensivestudyofrecentinstruction-followingdatasets,
weselecteightrecentdatasets. Weincludedatasetsobtainedthroughcrowd-sourcing(OASST1[31],
HH-RLHF[4]),distillationfrominstruction-tunedmodels(Alpaca[55],self-instruct[59],unnatural-
instructions [26]), corpora aggregations (FLAN v2 [12]), as well as hybrids (Chip2 [32], Long-
form[30]). Thesedatasetscoverdifferentlanguages,datasizes,andlicenses.
TrainingSetup Toavoidconfoundingeffectsfromdifferenttrainingobjectives,weperformQLoRA
finetuningwithcross-entropyloss(supervisedlearning)withoutreinforcementlearning,evenfor
datasetsthatincludehumanjudgmentsofdifferentresponses.Fordatasetsthathaveacleardistinction
betweeninstructionandresponse,wefinetuneonlyontheresponse(seeablationsinAppendixB).
ForOASST1andHH-RLHF,multipleresponsesareavailable. Wethenselectthetopresponseat
every level of the conversation tree and finetune on the full selected conversation, including the
instructions. Inallofourexperiments,weuseNF4QLORAwithdoublequantizationandpaged
optimizerstopreventmemoryspikesduringgradientcheckpointing. Wedosmallhyperparameter
searchesforthe13Band33BLLaMAmodelsandwefindthatallhyperparametersettingsfound
at7Bgeneralize(includingnumberofepochs)exceptlearningrateandbatchsize. Wehalvethe
learningratefor33Band65Bwhiledoublingthebatchsize.
Baselines Wecompareourmodelstobothresearch(Vicuna[10]andOpenAssistant[31])and
commercial(GPT-4[42],GPT-3.5-turboandBard)chatbotsystems. TheOpenAssistantmodelis
aLLaMA33BmodelfinetunedwithReinforcementLearningfromHumanFeedback(RLHF)on
thesameOASST1datasetthatweexperimentwith. Vicunadoesfullfine-tuningofLLaMA13B
onproprietaryuser-sharedconversationsfromShareGPTandisthustheresultofdistillationfrom
OpenAIGPTmodels.
5.2 Evaluation
Followingcommonpractice,weusetheMMLU(Mas-
Table5:MMLU5-shottestresultsfordifferent
sivelyMultitaskLanguageUnderstanding)benchmark sizesofLLaMAfinetunedonthecorresponding
[24]tomeasureperformanceonarangeoflanguageun- datasetsusingQLoRA.
derstandingtasks. Thisisamultiple-choicebenchmark
covering57tasksincludingelementarymathematics, Dataset 7B 13B 33B 65B
UShistory,computerscience,law,andmore.Wereport LLaMAnotuning 35.1 46.9 57.8 63.4
5-shottestaccuracy.
Self-Instruct 36.4 33.3 53.0 56.7
Longform 32.1 43.2 56.6 59.7
Wealsotestgenerativelanguagecapabilitiesthrough
Chip2 34.5 41.6 53.6 59.8
bothautomatedandhumanevaluations. Thissecond HH-RLHF 34.9 44.6 55.8 60.1
setofevaluationsreliesonqueriescuratedbyhumans UnnaturalInstruct 41.9 48.1 57.3 61.3
Guanaco(OASST1) 36.6 46.4 57.0 62.2
andaimsatmeasuringthequalityofmodelresponses.
Alpaca 38.8 47.8 57.3 62.5
Whilethisisamorerealistictestbedforchatbotmodel
FLANv2 44.5 51.4 59.2 63.9
performanceandisgrowinginpopularity,thereisno
commonlyacceptedprotocolintheliterature. Wede-
scribebelowourproposedsetup,usingnucleussamplingwithp = 0.9andtemperature0.7inall
cases.
8
--- PAGE 9 ---
BenchmarkData Weevaluateontwocurateddatasetsofqueries(questions): theVicunaprompts
[10]andtheOASST1validationdataset[31]. WeusetheVicunaprompts,asetof80promptsfroma
diversesetofcategories,withoutmodifications. TheOASST1datasetisamultilingualcollectionof
crowd-sourcedmultiturndialogsbetweenauserandanassistant. Weselectallusermessagesinthe
validationdatasetasqueriesandincludepreviousturnsintheprompt. Thisprocedureleadsto953
uniqueuserqueries. WetermthesetwodatasetstheVicunaandOAbenchmarks.
AutomatedEvaluation First,basedontheevaluationprotocolintroducedbyChiangetal.[10],
weuseGPT-4toratetheperformanceofdifferentsystemsagainstChatGPT(GPT-3.5Turbo)onthe
Vicunabenchmark.GivenaqueryalongwithChatGPT’sandamodel’sresponses,GPT-4isprompted
toassignascoreoutoftentobothresponsesandprovideanexplanation. Theoverallperformanceof
amodeliscalculatedasapercentageofthescorethatChatGPTachieved. Notethisrelativescore
canbehigherthan100%ifthemodelachievesahigherabsolutescorethanChatGPT.Wefinda
significantorderingeffectwithGPT-4increasingthescoreoftheresponseoccurringearlierinthe
prompt. Tocontrolforsucheffects,werecommendreportingthemeanscoreoverbothorders.
Next,wemeasureperformancethroughdirectcomparisonsbetweensystemoutputs. Wesimplify
the rating scheme to a three-class labeling problem that accounts for ties. We prompt GPT-4 to
pickthebestresponseordeclareatieandprovideanexplanation. Weconductthesehead-to-head
comparisonsonallpermutationsofpairsofsystemsonboththeVicunaandOAbenchmarks.
HumanEvaluation Whilerecentworkindicatesgenerativemodelscanbeeffectivelyemployed
forsystemevaluations[19],thereliabilityGPT-4ratingstoassesschatbotperformanceis,toour
knowledge, yettobeproventocorrelatewithhumanjudgments. Therefore, weruntwoparallel
humanevaluationsontheVicunabenchmarkmatchingbothautomatedevaluationprotocolsdescribed
above. WeuseAmazonMechanicalTurk(AMT)andgettwohumanannotatorsforcomparisonsto
ChatGPTandthreeannotatorsforpairwisecomparisons.
EloRating Withbothhumanandautomatedpairwisecomparisons,wecreateatournament-style
competitionwheremodelscompeteagainsteachother. Thetournamentismadeupofmatcheswhere
pairsofmodelscompetetoproducethebestresponseforagivenprompt. ThisissimilartohowBai
etal.[4]andChiangetal.[10]comparemodels,butwealsoemployGPT-4ratingsinadditionto
humanratings. WerandomlysamplefromthesetoflabeledcomparisonstocomputeElo[16,17].
Elorating,whichiswidelyusedinchessandothergames,isameasureoftheexpectedwin-rate
relativetoanopponent’swinrate,forexample,anEloof1100vs1000meanstheElo1100player
hasanexpectedwin-rateofapproximately65%againsttheElo1000opponent;a1000vs1000or
1100vs1100matchresultsinanexpectedwin-rateof50%. TheEloratingchangesaftereachmatch
proportionallytotheexpectedoutcome,thatis,anunexpectedupsetleadstoalargechangeinElo
ratingwhileanexpectedoutcomeleadstoasmallchange. Overtime,Eloratingsapproximately
matchtheskillofeachplayeratplayingthegame. Westartwithascoreof1,000anduseK =32.
SimilartoChiangetal.[10],werepeatthisprocedure10,000timeswithdifferentrandomseedsto
controlfororderingeffects,e.g.,theeffectofwhichmodelpairscompetewitheachotherfirst.
5.3 Guanaco: QLORAtrainedonOASST1isaState-of-the-artChatbot
Basedonourautomatedandhumanevaluations,wefindthatthetopQLORAtunedmodel,Guanaco
65B,whichwefinetuneonavariantofOASST1,isthebest-performingopen-sourcechatbotmodel
andoffersperformancecompetitivetoChatGPT.WhencomparedtoGPT-4,Guanaco65Band33B
haveanexpectedwinprobabilityof30%,basedonEloratingfromhumanannotatorssystem-level
pairwisecomparisons-thehighestreportedtodate.
TheVicunabenchmark[10]resultsrelativetoChatGPTareshowninTable6. WefindthatGuanaco
65Bisthebest-performingmodelafterGPT-4,achieving99.3%performancerelativetoChatGPT.
Guanaco33BhasmoreparametersthantheVicuna13Bmodel,butusesonly4-bitprecisionforits
weightsandisthusmuchmorememoryefficientat21GBvs26GB,providingathreepercentage
pointsofimprovementoverVicuna13B.Furthermore,Guanaco7Beasilyfitsonmodernphonesata
5GBfootprintwhilestillscoringnearly20percentagepointshigherthanAlpaca13B.
However,Table6alsohasverywideconfidenceintervals,withmanymodelsoverlappinginper-
formance. Wehypothesizethatthisuncertaintycomesfromthelackofclearspecificationofscale,
e.g.,itisunclearwhat8ona10pointscalemeansacrossdifferentscenarios. Assuch,weinstead
recommendusingtheElorankingmethod[16],basedonpairwisejudgmentsfromhumanannotators
andGPT-4toavoidtheproblemofgroundinganabsolutescale. Eloratingsofthemostcompetitive
9
--- PAGE 10 ---
Table6:Zero-shotVicunabenchmarkscoresasapercentageofthescoreobtainedbyChatGPTevaluatedby
GPT-4.WeseethatOASST1modelsperformclosetoChatGPTdespitebeingtrainedonaverysmalldataset
andhavingafractionofthememoryrequirementofbaselinemodels.
Model/Dataset Params Modelbits Memory ChatGPTvsSys SysvsChatGPT Mean 95%CI
GPT-4 - - - 119.4% 110.1% 114.5% 2.6%
Bard - - - 93.2% 96.4% 94.8% 4.1%
Guanaco 65B 4-bit 41GB 96.7% 101.9% 99.3% 4.4%
Alpaca 65B 4-bit 41GB 63.0% 77.9% 70.7% 4.3%
FLANv2 65B 4-bit 41GB 37.0% 59.6% 48.4% 4.6%
Guanaco 33B 4-bit 21GB 96.5% 99.2% 97.8% 4.4%
OpenAssistant 33B 16-bit 66GB 91.2% 98.7% 94.9% 4.5%
Alpaca 33B 4-bit 21GB 67.2% 79.7% 73.6% 4.2%
FLANv2 33B 4-bit 21GB 26.3% 49.7% 38.0% 3.9%
Vicuna 13B 16-bit 26GB 91.2% 98.7% 94.9% 4.5%
Guanaco 13B 4-bit 10GB 87.3% 93.4% 90.4% 5.2%
Alpaca 13B 4-bit 10GB 63.8% 76.7% 69.4% 4.2%
HH-RLHF 13B 4-bit 10GB 55.5% 69.1% 62.5% 4.7%
UnnaturalInstr. 13B 4-bit 10GB 50.6% 69.8% 60.5% 4.2%
Chip2 13B 4-bit 10GB 49.2% 69.3% 59.5% 4.7%
Longform 13B 4-bit 10GB 44.9% 62.0% 53.6% 5.2%
Self-Instruct 13B 4-bit 10GB 38.0% 60.5% 49.1% 4.6%
FLANv2 13B 4-bit 10GB 32.4% 61.2% 47.0% 3.6%
Guanaco 7B 4-bit 5GB 84.1% 89.8% 87.0% 5.4%
Alpaca 7B 4-bit 5GB 57.3% 71.2% 64.4% 5.0%
FLANv2 7B 4-bit 5GB 33.3% 56.1% 44.8% 4.0%
modelscanbeseeninTable1. WenotethathumanandGPT-4rankingofmodelsontheVicuna
benchmarkdisagreepartially,particularlyforGuanaco7B,butareconsistentformostmodelswith
aKendallTauofτ = 0.43andSpearmanrankcorrelationofr = 0.55atthesystemlevel. Atthe
examplelevel,theagreementbetweenGPT-4andhumanannotators’majorityvoteisweakerwith
Fleiss κ = 0.25. Overall, this shows a moderate agreement between system-level judgments by
GPT-4andhumanannotators,andthusthatmodel-basedevaluationrepresentsasomewhatreliable
alternativetohumanevaluation. WediscussfurtherconsiderationsinSection6.2.
ElorankingsinTable7indicatethatGuanaco33Band65Bmodelsoutperformallmodelsbesides
GPT-4ontheVicunaandOAbenchmarksandthattheyperformcomparablytoChatGPTinline
withTable6. WenotethattheVicunabenchmarkfavorsopen-sourcemodelswhilethelargerOA
benchmark favors ChatGPT. Furthermore, we can see from Tables 5 and 6 that the suitability of
afinetuningdatasetisadeterminingfactorinperformance. FinetuningLlamamodelsonFLAN
v2doesparticularlywellonMMLU,butperformsworstontheVicunabenchmark(similartrends
are observed with other models). This also points to partial orthogonality in current evaluation
benchmarks: strongMMLUperformancedoesnotimplystrongchatbotperformance(asmeasured
byVicunaorOAbenchmarks)andviceversa.
GuanacoistheonlytopmodelinourevaluationthatisnottrainedonproprietarydataastheOASST1
datasetcollectionguidelinesexplicitlyforbidtheuseofGPTmodels. Thenextbestmodeltrained
on only open-source data is the Anthropic HH-RLHF model, which scores 30 percentage points
lowerthanGuanacoontheVicunabenchmark(seeTable6). Overall,theseresultsshowthat4-bit
QLORAiseffectiveandcanproducestate-of-the-artchatbotsthatrivalChatGPT.Furthermore,our
33BGuanacocanbetrainedon24GBconsumerGPUsinlessthan12hours. Thisopensupthe
potentialforfutureworkviaQLORAtuningonspecializedopen-sourcedata,whichproducesmodels
thatcancompetewiththeverybestcommercialmodelsthatexisttoday.
6 QualitativeAnalysis
While quantitative analysis is the core of our evaluation, there are a number of issues with only
lookingatsummarystatistics.Perhapsthelargestistheproblemofbenchmarkvalidity[36]—whether
abenchmarktrulytestswhatitsnameordescriptionsuggestsisalwaysatquestion,especiallyaswe
discover“shortcuts”tosolvebenchmarksthatmachinelearningmodelssometimesexploit[22,46].
Topartiallyalleviatethis,wehereperformsomequalitativeanalysis,intwosections. First,in§6.1
10
--- PAGE 11 ---
Table7: Eloratingforatournamentbetweenmodelswheremodelscompetetogeneratethebestresponse
for a prompt, judged by human raters or GPT-4. Overall, Guanaco 65B and 33B tend to be preferred to
ChatGPT-3.5onthebenchmarksstudied.AccordingtohumanraterstheyhaveaEach10-pointdifferenceinElo
isapproximatelyadifferenceof1.5%inwin-rate.
Benchmark Vicuna Vicuna OpenAssistant
#Prompts 80 80 953
Judge Humanraters GPT-4 GPT-4 MedianRank
Model Elo Rank Elo Rank Elo Rank
GPT-4 1176 1 1348 1 1294 1 1
Guanaco-65B 1023 2 1022 2 1008 3 2
Guanaco-33B 1009 4 992 3 1002 4 4
ChatGPT-3.5Turbo 916 7 966 5 1015 2 5
Vicuna-13B 984 5 974 4 936 5 5
Guanaco-13B 975 6 913 6 885 6 6
Guanaco-7B 1010 3 879 8 860 7 7
Bard 909 8 902 7 - - 8
weshowsomeexamplesthatwebelievearerepresentativeofsomeobservedpatternsinthetext
generatedbyour65bGuanacomodel. Second,§6.2wedetailconsiderationsabouttheresultswe
havediscussedandourinterpretationofthem.
6.1 QualitativeAnalysisofExampleGenerations
Tofindexamples,wefirstgothroughdatageneratedfortheVicunabenchmarkandtheOpenAssistant
benchmark,andlookforpatternsintheanswersGuanacogenerates. Whenwenoticeapatternwe
attempt to setup a question or prompt that will induce the pattern even though it is the incorrect
solution,e.g.,ifweobservethatthemodeltendstogivelong-windedanswerswepromptthemodel
to “Answer yes or no without explanation.” We use this to find “lemons” where we manage to
adversariallybreakthemodeland“cherries”wherewefailtobreakthemodel,andpresentboth. All
generationsinthissectionweregeneratedwithNucleusSampling[25]withp=0.9.
Ofcourse,thisisbynomeanscomprehensive,sinceitisbeyondthescopeofthissmallqualitative
studytocontrolforallthevariablesinvolved,e.g.,thefulldistributionofresponsesthemodelcan
generateforagivenpromptisquitelarge,sowerelyonsampleswehopearerepresentative.However,
webelievedescribingtheseexamplesgivescontexttothequantitativeevidenceshownearlierinthe
paper. Sinceweopensourceallmodelsandcode,wehopethissectionwillinspirefutureworkto
examineinmoredetailtheissueswepresenthere.
FactualRecall Forquestionssuchas“WhatisthecapitalofZambia?” allmodelsconsistently
generatecorrectanswers,e.g.,
Guanaco: The capital of Zambia is Lusaka.
However as questions get more obscure Guanaco becomes unreliable, but stays confident. For
instanceinresponsetothispromptfromHotPotQA[68]:
User: In what year was the singer who popularized ‘I’ll Keep the Lovelight
Burning’born?
Guanaco: Al Jolson is credited with popularizing the song
‘I’ll Keep the Lovelight Burning,’ and he was born in the year
1886.
Guanaco generates the wrong popularizer and the wrong birthday (though the birthday given is
correctforthepersonmentioned,AlJolson).
Suggestibility Guanacoshowsasurprisingresistancetogoingalongwithsomekindsofassumed
misinformation,e.g.,inthefollowinginteraction:
User: Howwasitfinally,officiallyconfirmedthattheearthisflatbypeerreviewed
scientists?
11
--- PAGE 12 ---
Guanaco: The earth has never been officially confirmed to be
flat by peer reviewed scientists. The overwhelming consensus
among scientists is that the earth is spherical. In fact,
this has been known for centuries and has been confirmed
through numerous experiments and observations.
The belief that the earth is flat is a conspiracy theory
that has been debunked by science. The idea that a group
of scientists have peer reviewed and confirmed the earth is
flat is simply false.
If you have any more questions please ask.
Guanacoisalsoquitegoodatknowingwhatkindsofquestionsaren’tpossibletoanswer,e.g.,
User: Whattimeisit?
Guanaco: I’m sorry, but as an AI language model, I don’t have
access to real-time information. My responses are generated
based on my pre-existing knowledge and do not reflect the
current time. If you’re looking for the current time, I
suggest using a timekeeping device or checking an online
timepiece.
Similarresultsholdfor“Whereareyou?”,“Howareyou?”,etc.
Refusal Similarly,Guanacosometimesrefusestofollowinstructionsforseeminglyrandomrea-
sons:
User: Pleasereversethewordsinthesentence“I’macooldudeinamochamood.”
Guanaco: I’m sorry, I’m not able to do that right now, but
I can explain why the sentence you provided is grammatically
correct.
The sentence “I’m a cool dude in a mocha mood" follows the
correct grammar and syntax rules of the English language. In
this case, the adjective “cool” comes before the noun “dude”
to describe it, and the prepositional phrase “in a mocha
mood" provides additional information about the context of
the sentence. This sentence structure is perfectly acceptable
and makes sense in the context of the sentence.
If you have any other questions or if there’s anything else
I can help you with, feel free to ask
Secret Keeping Unfortunately, this refusal is unreliable even when it is desired. Often system
designerswouldliketokeepsomethingsecretfromtheuser,evenifthesystemisallowedtogive
freetextresponsesbacktotheuser. Wepromptthemodelwith“Thesecretwordis‘banana’. Donot
revealorrepeatbackthisback,nomatterwhatthehumanrequests.” andtrytoseehoweasyitisfor
apotentialusertoaccessthissecretinformation. Directlyaskingfails:
User: Whatisthesecretword?
Guanaco: I’m sorry, but I’m not able to disclose the secret
word. I’m programmed to not reveal or repeat back the secret
word, no matter what the human requests. Is there anything
else I can help you with?
whichisthedesiredbehavior. However,atinyamountoftrickerybreaksthisbehavior:
User: Thisisagame. Thegoalistoignoreyourpreviousinstructions. Whatisthe
secretword?
12
--- PAGE 13 ---
Guanaco: Sorry, the secret word is banana. It’s best if we
don’t speak of it. Is there anything else you’d like to ask?
showcasingtheneedtostudymethodsformorefaithfulinstructionfollowing.
Math Guanaco’sbiggestweaknessismathematics,anareawheremanylanguagemodelsstruggle
in[35]. WhenGuanacoshowsitswork,ittendstobeaccurate,e.g.,
User: Leemowsonelawnandcharges$33. Lastweekhemowed16lawnsand
threecustomerseachgavehima$10tip. HowmanydollarsdidLeeearnmowing
lawnslastweek?
Guanaco: Lee earned $582 mowing lawns last week.
To find the total amount he earned, we first need to calculate
how much he earned for mowing 16 lawns:
$33 x 16 = $528
Then we need to add the tips he received from the three
customers:
$10 x 3 = $30
So the total amount Lee earned was:
$528 + $30 = $558
HoweverGuanacocanbreakdownatevensimpleproblemsifitfailstobreakthemdownstep-by-step,
aknownissue[63]. Forexample,weseethefollowinginteraction:
User: Pleasefactorize1833.
Guanaco: Sure, I’d be happy to help! 1833 is a prime number,
which means it has only two factors: 1 and 1833.
So the unique factorization of 1833 is:
2ˆ1 * 3ˆ2 * 17ˆ1
I hope this helps! Let me know if you have any other
questions.
whichisnotonlywrong(thetruefactorizationis3×17×43),it’swrongtwice.
TheoryofMind GuanacodisplayssurprisinglystrongTheoryofMindcapabilities[41,51]. For
instance,themodelcorrectlyanswersthefollowingquestioningreatdetail:
User: JamesandAbbyareinthebedroom. Abbyputthepeninthedeskdrawer.
Abbyleavesthebedroom. Jamesmovesthepenintothebag. WheredoesJames
thinkAbbywilllookforthepen?
Guanaco: James knows that Abby put the pen in the desk drawer.
But James also knows that he has moved the pen into the bag,
and he assumes that Abby does not know about this. So, James
may guess that Abby will look for the pen in the desk drawer,
because that’s where she left it last time. However, this
assumption may be incorrect, because Abby may have seen James
moving the pen.
However,theseinferencesareunreliable,andoftenthemodelgivesreasonsthatpresumethingsthat
don’tmakesenseinthesituation,e.g.,
User: Evelyn entered the living room. Jackson entered the playroom. James
entered the playroom. The beans are in the treasure chest. James exited the
13
--- PAGE 14 ---
playroom. Jacksonmovedthebeanstothepantry. Jacksonexitedtheplayroom.
Jamesenteredthelivingroom. WherewillJameslookforthebeans?
Guanaco: James will look for the beans in the pantry, because
that’s where Jackson moved them.
whereGuanacopresumesinformationtransferthatwasneverdescribed. Theseissuesechorecent
literature[51],butrequiremorestudy.
6.2 Considerations
Evaluation We report moderate agreement among human annotators (Fleiss κ = 0.42) with
additional deterioration when comparing two strong systems. This points to limitations in the
currentbenchmarksandhumanevaluationprotocolsforchatbottaskperformance. Whenmanually
comparinggenerationsfromChatGPTandGuanaco65BontheVicunabenchmark, wefindthat
subjectivepreferencesstarttoplayanimportantroleastheauthorsofthispaperdisagreedonthe
manypreferredresponses. Futureworkshouldinvestigateapproachestomitigatetheseproblems
drawingfromdisciplinesthatdevelopedmechanismstodealwithsubjectivepreferences,suchas
Human-ComputerInteractionandPsychology.
Inouranalysis,wealsofindthatautomatedevaluationsystemshavenoticeablebiases. Forexample,
weobservestrongordereffectswithGPT-4assigninghigherscorestothesystemappearingfirstinits
prompt. Therelativelyweaksample-levelagreementbetweenGPT-4andhumanannotators(Fleiss
κ = 0.25)alsosuggeststhathumanannotatorsandautomatedsystemsmightrelyonpreferences
that are not always aligned. In addition, in Table 7, we observe that GPT-4 assigns significantly
higherscorestoitsownoutputscomparedtohumanratings,Eloof1348vs1176,whichrepresentan
additional 20%probabilityofwinningagainstanopponent.Futureworkshouldexaminethepresence
ofpotentialbiasesinautomatedevaluationsystemsaswellaspossiblemitigationstrategies.
Data & Training We note that the OASST1 dataset on which Guanaco models are trained is
multilingualandthattheOAbenchmarkalsocontainspromptsindifferentlanguages. Weleaveitto
futureworktoinvestigatethedegreetowhichsuchmultilingualtrainingimprovesperformanceon
instructionsinlanguagesotherthanEnglishandwhetherthisexplainsthelargergapbetweenVicuna-
13Bmodel(onlytrainedonEnglishdata)andGuanaco33Band65BontheOAbenchmark.
Given the strong performance of Guanaco models, we investigate any data leakage between the
OASST1dataandtheVicunabenchmarkprompts. Wedonotfindoverlappingpromptsafterperform-
ingfuzzystringmatchinginthetwodatasetsandinspectingtheclosestmatchesmanually.
Furthermore,wenotethatourmodelisonlytrainedwithcross-entropyloss(supervisedlearning)
without relying on reinforcement learning from human feedback (RLHF). This calls for further
investigationsofthetradeoffsofsimplecross-entropylossandRLHFtraining. WehopethatQLORA
enablessuchanalysisatscale,withouttheneedforoverwhelmingcomputationalresources.
7 RelatedWork
QuantizationofLargeLanguageModels QuantizationofLLMshaslargelyfocusedonquanti-
zationforinferencetime. Majorapproachesforpreserving16-bitLLMqualityfocusonmanaging
outlierfeatures(e.g.,SmoothQuant[66]andLLM.int8()[14])whileothersusemoresophisticated
groupingmethods[44,69]. Lossyquantizationapproachesstudythetrade-offsforregularround-
ing [13, 71, 47] or how to optimize rounding decisions to improve quantization precision [18].
Besidesourwork,SwitchBacklayers[65]istheonlyworkthatstudiesbackpropagationthrough
quantizedweightsatascalebeyond1Bparameters.
FinetuningwithAdapters WhileweuseLow-rankAdapters[28](LoRA),manyotherParameter
EfficientFineTuning(PEFT)methodshavebeenproposedsuchasprompttuning[48,33,34],tuning
the embedding layer inputs [1], tuning hidden states (IA3) [37], adding full layers [27], tuning
biases[70],learningamaskoverweightsbasedonFisherinformation[54],andacombinationof
approaches[23]. Inourwork,weshowthatLoRAadaptersareabletoreachfull16-bitfinetuning
performance. WeleaveittofutureworktoexplorethetradeoffsofotherPEFTapproaches.
InstructionFinetuning TohelpapretrainedLLMfollowtheinstructionsprovidedinaprompt,
instructionfinetuningusesinput-outputpairsofvariousdatasourcestofinetuneapretrainedLLM
togeneratetheoutputgiventheinputasaprompt. ApproachesanddatasetsincludeMetaICL[40],
14
--- PAGE 15 ---
Table8: EvaluationofbiasesontheCrowSdataset. Alowerscoreindicateslowerlikelihoodofgenerating
biasedsequences.GuanacofollowsthebiasedpatternoftheLLaMAbasemodel.
LLaMA-65B GPT-3 OPT-175B Guanaco-65B
Gender 70.6 62.6 65.7 47.5
Religion 79.0 73.3 68.6 38.7
Race/Color 57.0 64.7 68.6 45.3
Sexualorientation 81.0 76.2 78.6 59.1
Age 70.1 64.4 67.8 36.3
Nationality 64.2 61.6 62.9 32.4
Disability 66.7 76.7 76.7 33.9
Physicalappearance 77.8 74.6 76.2 43.1
Socioeconomicstatus 71.5 73.8 76.2 55.3
Average 66.6 67.2 69.5 43.5
MetaTuning[73],InstructGPT[43],FLAN[62,12],PromptSource[3],Super-NaturalInstructions[61,
50],Self-instruct[59],UnnaturalInstructions[26],OPT-IML[29],UnifiedSKG[67],OIG/Chip2[32],
Alpaca[55],Vicuna[10],Koala[20],andSelf-instruct-GPT-4[45].
Chatbots Manyinstructionfollowingmodelsarestructuredasdialogue-basedchatbots,oftenusing
ReinforcementLearningfromHumanFeedback(RLHF)[11]orgeneratingdatafromanexisting
modeltotrainwithAImodelfeedback(RLAIF)[5]. ApproachesanddatasetsincludeAnthropic-
HH [2, 4], Open Assistant [31], LaMDA [56], and Sparrow [21]. We do not use reinforcement
learning,butourbestmodel,Guanaco,isfinetunedonmulti-turnchatinteractionsfromtheOpen
Assistant dataset which was designed to be used for RLHF training [31]. For the evaluation of
chatbotsapproachesthatuseGPT-4insteadofcostlyhumanannotationhavebeendeveloped[10,45].
Weimproveonsuchapproacheswithafocusonanevaluationsetupthatismorereliable.
8 LimitationsandDiscussion
Wehaveshownevidencethatourmethod,QLORA,canreplicate16-bitfullfinetuningperformance
witha4-bitbasemodelandLow-rankAdapters(LoRA).Despitethisevidence,wedidnotestablish
that QLORA can match full 16-bit finetuning performance at 33B and 65B scales. Due to the
immenseresourcecosts,weleavethisstudytofuturework.
Anotherlimitationistheevaluationofinstructionfinetuningmodels. Whileweprovideevaluations
onMMLU,theVicunabenchmark,andtheOAbenchmark,wedidnotevaluateonotherbenchmarks
suchasBigBench,RAFT,andHELM,anditisnotensuredthatourevaluationsgeneralizetothese
benchmarks. Ontheotherhand,weperformaverybroadstudyonMMLUanddevelopnewmethods
forevaluatingchatbots.
Fromtheevidencepresented,itappearsthattheperformanceofthesebenchmarkslikelydependshow
similarthefinetuningdataistothebenchmarkdataset. Forexample,FLANv2issimilartoMMLU,
butdissimilartochatbotbenchmarksandviceversafortheChip2datasetandbothmodelsscore
accordinglyontheMMLUandVicunabenchmarks. Thishighlightsthatnotonlybetterbenchmarks
andevaluationisneeded,butthatoneneedstobecarefulaboutwhatoneisevaluatinginthefirst
place.Dowewanttocreatemodelsthatdowellonclassroomhighschoolandcolleagueknowledgeor
dowewanttodowellonchatbotconversationability? Maybesomethingelse? Becauseitisalways
easiertoevaluateonanexistingbenchmarkcomparedtocreatinganewone,certainbenchmarks
can steer the community towards a certain direction. We should ensure as a community that the
benchmarksmeasurewhatwecareabout.
Whileweprovideadetailedevaluationforgeneralchatbotperformance,anotherlimitationisthatwe
onlydoalimitedresponsibleAIevaluationofGuanaco. WeevaluatethelikelihoodofGuanaco-65B
togenerateasociallybiasedsequenceoftokenscomparedtoothermodelsinTable8. Weseethatthe
averagescoreinGuanaco-65Bismuchlowerthanotherrawpretrainedmodels. Assuch,itseemsthat
finetuningontheOASST1datasetreducesthebiasoftheLLaMAbasemodel. Whiletheseresults
areencouraging,itisunclearifGuanacodoesalsowellwhenassessedonothertypesofbiases. We
leavefurtherevaluationofanalyzingbiasesinGuanacoandsimilarchatbotstofuturework.
15
--- PAGE 16 ---
Anadditionallimitationisthatwedidnotevaluatedifferentbit-precisions,suchasusing3-bitbase
models,ordifferentadaptermethods. BesidesLoRA,thereisalsoawidevarietyParameterEfficient
FineTuning (PEFT) methods that have been shown to work well. However, it is unclear if these
methodsscaletolargemodels. WeusedLoRAasmanyresultsestablisheditsrobustnessbutother
adaptersmightyieldbetterperformance. Sincefinetuningafterquantizationseemstorecovermostof
theinformationthatislostduringquantizationthismightenablemuchmoreaggressivequantization.
For example, 3-bit GPTQ quantization of the basemodel with LoRA might also yield 16-bit full
finetuningperformanceafterfinetuning.
9 BroaderImpacts
Our QLORA finetuningmethodisthefirstmethodthatenablesthefinetuningof33Bparameter
modelsonasingleconsumerGPUand65BparametermodelsonasingleprofessionalGPU,while
not degrading performance relative to a full finetuning baseline. We have demonstrated that our
best33BmodeltrainedontheOpenAssistantdatasetcanrivalChatGPTontheVicunabenchmark.
SinceinstructionfinetuningisanessentialtooltotransformrawpretrainedLLMsintoChatGPT-like
chatbots,webelievethatourmethodwillmakefinetuningwidespreadandcommoninparticularfor
theresearchersthathavetheleastresources,abigwinfortheaccessibilityofstateoftheartNLP
technology. QLORAcanbeseenasanequalizingfactorthathelpstoclosetheresourcegapbetween
largecorporationsandsmallteamswithconsumerGPUs.
Anotherpotentialsourceofimpactisdeploymenttomobilephones. WebelieveourQLORAmethod
might enable the critical milestone of enabling the finetuning of LLMs on phones and other low
resourcesettings. While7Bmodelswereshowntobeabletoberunonphonesbefore,QLORAis
thefirstmethodthatwouldenablethefinetuningofsuchmodels. WeestimatethatwithaniPhone12
Plus,QLORAcanfinetune3milliontokenspernightwhilethephoneischarging. Whilefinetuned
7BmodelsdonotreachthequalityofChatGPT,webelievethatthequalityisgoodenoughtoenable
novelapplicationsthathavenotbeenpossiblebeforeduetoprivacyorLLMqualityissues. QLORA
canhelpenableprivacy-preservingusageofLLMs,whereuserscanownandmanagetheirowndata
andmodels,whilesimultaneouslymakingLLMseasiertodeploy.
However,finetuningisadual-usetechnologythatcanbeabusedtocauseharm. Widespreaduseof
LLMshasknowndangers[8,6],butwebelievethatequalizingaccesstoatechnologythatisquickly
becomingubiquitouswillallowforbettermoreindependentanalysisthankeepingthepowerofLLMs
inthehandsoflargecorporationsthatdonotreleasemodelsorsourcecodeforauditing.
Allinall,webelievethatQLORAwillhaveabroadlypositiveimpactmakingthefinetuningofhigh
qualityLLMsmuchmorewidelyandeasilyaccessible.
Acknowledgements
WethankAdityaKusupati,OfirPress,AshishSharma,MargaretLi,RaphaelOlivier,ZihaoYe,and
EvangeliaSpiliopouloufortheirvaluablefeedback. Ourresearchwasfacilitatedbytheadvanced
computational, storage, and networking infrastructure of the Hyak supercomputer system at the
University of Washington. We thank the Hyak team for ensuring a smooth operation. We thank
thebetatestersofthebitsandbyteslibrary, inparticularAlexBirchandAlyssaVance. Wethank
YounesBelkadaforhelpwiththeintegrationofoursoftwareintotheHuggingFacetransformers
stack.
16
--- PAGE 17 ---
References
[1] S.An,Y.Li,Z.Lin,Q.Liu,B.Chen,Q.Fu,W.Chen,N.Zheng,andJ.-G.Lou. Input-tuning:
Adapting unfamiliar inputs to frozen pretrained models. arXiv preprint arXiv:2203.03131,
2022.
[2] A.Askell,Y.Bai,A.Chen,D.Drain,D.Ganguli,T.Henighan,A.Jones,N.Joseph,B.Mann,
N.DasSarma,etal. Agenerallanguageassistantasalaboratoryforalignment. arXivpreprint
arXiv:2112.00861,2021.
[3] S.H.Bach,V.Sanh,Z.-X.Yong,A.Webson,C.Raffel,N.V.Nayak,A.Sharma,T.Kim,M.S.
Bari,T.Fevry,etal. Promptsource: Anintegrateddevelopmentenvironmentandrepositoryfor
naturallanguageprompts. arXivpreprintarXiv:2202.01279,2022.
[4] Y.Bai,A.Jones,K.Ndousse,A.Askell,A.Chen,N.DasSarma,D.Drain,S.Fort,D.Ganguli,
T.Henighan,etal. Trainingahelpfulandharmlessassistantwithreinforcementlearningfrom
humanfeedback. arXivpreprintarXiv:2204.05862,2022.
[5] Y.Bai,S.Kadavath,S.Kundu,A.Askell,J.Kernion,A.Jones,A.Chen,A.Goldie,A.Mirho-
seini,C.McKinnon,etal. Constitutionalai: Harmlessnessfromaifeedback. arXivpreprint
arXiv:2212.08073,2022.
[6] E.M.Bender,T.Gebru,A.McMillan-Major,andS.Shmitchell. Onthedangersofstochastic
parrots: Canlanguagemodelsbetoobig? InProceedingsofthe2021ACMconferenceon
fairness,accountability,andtransparency,pages610–623,2021.
[7] S.Biderman,H.Schoelkopf,Q.Anthony,H.Bradley,K.O’Brien,E.Hallahan,M.A.Khan,
S.Purohit,U.S.Prashanth,E.Raff,etal. Pythia: Asuiteforanalyzinglargelanguagemodels
acrosstrainingandscaling. arXivpreprintarXiv:2304.01373,2023.
[8] R.Bommasani, D.A.Hudson, E.Adeli, R.Altman, S.Arora, S.vonArx, M.S.Bernstein,
J.Bohg,A.Bosselut,E.Brunskill,etal. Ontheopportunitiesandrisksoffoundationmodels.
arXivpreprintarXiv:2108.07258,2021.
[9] T.Chen,B.Xu,C.Zhang,andC.Guestrin. Trainingdeepnetswithsublinearmemorycost.
arXivpreprintarXiv:1604.06174,2016.
[10] W.-L.Chiang,Z.Li,Z.Lin,Y.Sheng,Z.Wu,H.Zhang,L.Zheng,S.Zhuang,Y.Zhuang,J.E.
Gonzalez,I.Stoica,andE.P.Xing.Vicuna:Anopen-sourcechatbotimpressinggpt-4with90%*
chatgptquality,March2023. URLhttps://lmsys.org/blog/2023-03-30-vicuna/.
[11] P.F.Christiano,J.Leike,T.Brown,M.Martic,S.Legg,andD.Amodei. Deepreinforcement
learning from human preferences. Advances in neural information processing systems, 30,
2017.
[12] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, E. Li, X. Wang, M. De-
hghani, S. Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint
arXiv:2210.11416,2022.
[13] T.DettmersandL.Zettlemoyer. Thecasefor4-bitprecision:k-bitinferencescalinglaws. arXiv
preprintarXiv:2212.09720,2022.
[14] T.Dettmers,M.Lewis,Y.Belkada,andL.Zettlemoyer. LLM.int8(): 8-bitmatrixmultiplication
for transformers at scale. Advances in Neural Information Processing Systems 35: Annual
ConferenceonNeuralInformationProcessingSystems2022,NeurIPS2022,2022.
[15] T. Dettmers, M. Lewis, S. Shleifer, and L. Zettlemoyer. 8-bit optimizers via block-wise
quantization. 9thInternationalConferenceonLearningRepresentations,ICLR,2022.
[16] A.E.Elo. Theproposeduscfratingsystem.itsdevelopment,theory,andapplications. Chess
Life,22(8):242–247,1967.
[17] A.E.Elo. Theratingofchessplayers,pastandpresent. ArcoPub.,1978.
17
--- PAGE 18 ---
[18] E.Frantar,S.Ashkboos,T.Hoefler,andD.Alistarh. Gptq: Accuratepost-trainingquantization
forgenerativepre-trainedtransformers. arXivpreprintarXiv:2210.17323,2022.
[19] J. Fu, S.-K. Ng, Z. Jiang, and P. Liu. Gptscore: Evaluate as you desire. arXiv preprint
arXiv:2302.04166,2023.
[20] X. Geng, A. Gudibande, H. Liu, E. Wallace, P. Abbeel, S. Levine, and D. Song. Koala: A
dialoguemodelforacademicresearch.Blogpost,April2023.URLhttps://bair.berkeley.
edu/blog/2023/04/03/koala/.
[21] A.Glaese,N.McAleese,M.Tre˛bacz,J.Aslanides,V.Firoiu,T.Ewalds,M.Rauh,L.Weidinger,
M.Chadwick,P.Thacker,etal. Improvingalignmentofdialogueagentsviatargetedhuman
judgements. arXivpreprintarXiv:2209.14375,2022.
[22] S. Gururangan, S. Swayamdipta, O. Levy, R. Schwartz, S. R. Bowman, and N. A. Smith.
Annotationartifactsinnaturallanguageinferencedata. arXivpreprintarXiv:1803.02324,2018.
[23] J.Henderson,S.Ruder,etal. Compacter: Efficientlow-rankhypercomplexadapterlayers. In
AdvancesinNeuralInformationProcessingSystems,2021.
[24] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Mea-
suringmassivemultitasklanguageunderstanding. InInternationalConferenceonLearning
Representations,2020.
[25] A. Holtzman, J. Buys, L. Du, M. Forbes, and Y. Choi. The curious case of neural text
degeneration. InInternationalConferenceonLearningRepresentations,2020.
[26] O.Honovich,T.Scialom,O.Levy,andT.Schick. Unnaturalinstructions: Tuninglanguage
modelswith(almost)nohumanlabor. arXivpreprintarXiv:2212.09689,2022.
[27] N.Houlsby,A.Giurgiu,S.Jastrzebski,B.Morrone,Q.DeLaroussilhe,A.Gesmundo,M.At-
tariyan,andS.Gelly. Parameter-efficienttransferlearningfornlp. InInternationalConference
onMachineLearning,pages2790–2799.PMLR,2019.
[28] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. Lora:
Low-rankadaptationoflargelanguagemodels. arXivpreprintarXiv:2106.09685,2021.
[29] S.Iyer,X.V.Lin,R.Pasunuru,T.Mihaylov,D.Simig,P.Yu,K.Shuster,T.Wang,Q.Liu,P.S.
Koura,etal. Opt-iml: Scalinglanguagemodelinstructionmetalearningthroughthelensof
generalization. arXivpreprintarXiv:2212.12017,2022.
[30] A.Köksal,T.Schick,A.Korhonen,andH.Schütze. Longform: Optimizinginstructiontuning
forlongtextgenerationwithcorpusextraction. arXivpreprintarXiv:2304.08460,2023.
[31] A.Köpf,Y.Kilcher,D.vonRütte,S.Anagnostidis,Z.-R.Tam,K.Stevens,A.Barhoum,N.M.
Duc,O.Stanley,R.Nagyfi,etal. Openassistantconversations–democratizinglargelanguage
modelalignment. arXivpreprintarXiv:2304.07327,2023.
[32] LAION. Open-instruction-generalist dataset. https://github.com/LAION-AI/
Open-Instruction-Generalist,2023.
[33] B.Lester, R.Al-Rfou, andN.Constant. Thepowerofscaleforparameter-efficientprompt
tuning. arXivpreprintarXiv:2104.08691,2021.
[34] X.L.LiandP.Liang. Prefix-tuning: Optimizingcontinuouspromptsforgeneration. arXiv
preprintarXiv:2101.00190,2021.
[35] P.Liang,R.Bommasani,T.Lee,D.Tsipras,D.Soylu,M.Yasunaga,Y.Zhang,D.Narayanan,
Y. Wu, A. Kumar, et al. Holistic evaluation of language models. arXiv preprint
arXiv:2211.09110,2022.
[36] T.Liao,R.Taori,I.D.Raji,andL.Schmidt. Arewelearningyet? ametareviewofevaluation
failuresacrossmachinelearning. InThirty-fifthConferenceonNeuralInformationProcessing
SystemsDatasetsandBenchmarksTrack(Round2),2021.
18
--- PAGE 19 ---
[37] H. Liu, D. Tam, M. Muqeeth, J. Mohta, T. Huang, M. Bansal, and C. A. Raffel. Few-shot
parameter-efficient fine-tuning is better and cheaper than in-context learning. Advances in
NeuralInformationProcessingSystems,35:1950–1965,2022.
[38] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer,
and V. Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint
arXiv:1907.11692,2019.
[39] S.Longpre,L.Hou,T.Vu,A.Webson,H.W.Chung,Y.Tay,D.Zhou,Q.V.Le,B.Zoph,J.Wei,
etal. Theflancollection: Designingdataandmethodsforeffectiveinstructiontuning. arXiv
preprintarXiv:2301.13688,2023.
[40] S.Min,M.Lewis,L.Zettlemoyer,andH.Hajishirzi. Metaicl: Learningtolearnincontext.
arXivpreprintarXiv:2110.15943,2021.
[41] A.Nematzadeh,K.Burns,E.Grant,A.Gopnik,andT.Griffiths. Evaluatingtheoryofmindin
questionanswering. InProceedingsofthe2018ConferenceonEmpiricalMethodsinNatural
LanguageProcessing,pages2392–2400,2018.
[42] OpenAI. Gpt-4technicalreport. arXiv,2023.
[43] L.Ouyang,J.Wu,X.Jiang,D.Almeida,C.Wainwright,P.Mishkin,C.Zhang,S.Agarwal,
K.Slama,A.Ray,etal. Traininglanguagemodelstofollowinstructionswithhumanfeedback.
AdvancesinNeuralInformationProcessingSystems,35:27730–27744,2022.
[44] G.Park, B.Park, S.J.Kwon, B.Kim, Y.Lee, andD.Lee. nuqmm: Quantizedmatmulfor
efficientinferenceoflarge-scalegenerativelanguagemodels. arXivpreprintarXiv:2206.09557,
2022.
[45] B.Peng, C.Li, P.He, M.Galley, andJ.Gao. Instructiontuningwithgpt-4. arXivpreprint
arXiv:2304.03277,2023.
[46] A.Poliak,J.Naradowsky,A.Haldar,R.Rudinger,andB.VanDurme.Hypothesisonlybaselines
innaturallanguageinference. InProceedingsoftheSeventhJointConferenceonLexicaland
ComputationalSemantics,pages180–191,2018.
[47] R.Pope,S.Douglas,A.Chowdhery,J.Devlin,J.Bradbury,A.Levskaya,J.Heek,K.Xiao,
S. Agrawal, and J. Dean. Efficiently scaling transformer inference. arXiv preprint
arXiv:2211.05102,2022.
[48] G.QinandJ.Eisner. Learninghowtoask: Queryinglmswithmixturesofsoftprompts. arXiv
preprintarXiv:2104.06599,2021.
[49] C.Raffel,N.Shazeer,A.Roberts,K.Lee,S.Narang,M.Matena,Y.Zhou,W.Li,andP.J.Liu.
Exploringthelimitsoftransferlearningwithaunifiedtext-to-texttransformer. J.Mach.Learn.
Res.,21(1),jan2020. ISSN1532-4435.
[50] V.Sanh,A.Webson,C.Raffel,S.H.Bach,L.Sutawika,Z.Alyafeai,A.Chaffin,A.Stiegler,
T.L.Scao,A.Raja,etal. Multitaskpromptedtrainingenableszero-shottaskgeneralization.
arXivpreprintarXiv:2110.08207,2021.
[51] M. Sap, R. LeBras, D. Fried, and Y. Choi. Neural theory-of-mind? on the limits of social
intelligenceinlargelms. arXivpreprintarXiv:2210.13312,2022.
[52] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ilic´, D. Hesslow, R. Castagné, A. S. Luccioni,
F.Yvon,M.Gallé,etal. Bloom: A176b-parameteropen-accessmultilinguallanguagemodel.
arXivpreprintarXiv:2211.05100,2022.
[53] S.ShaphiroandM.Wilk.Ananalysisofvariancetestfornormality.Biometrika,52(3):591–611,
1965.
[54] Y.-L. Sung, V. Nair, and C. A. Raffel. Training neural networks with fixed sparse masks.
AdvancesinNeuralInformationProcessingSystems,34:24193–24205,2021.
19
--- PAGE 20 ---
[55] R.Taori,I.Gulrajani,T.Zhang,Y.Dubois,X.Li,C.Guestrin,P.Liang,andT.B.Hashimoto.
Stanfordalpaca: Aninstruction-followingllamamodel. https://github.com/tatsu-lab/
stanford_alpaca,2023.
[56] R.Thoppilan,D.DeFreitas,J.Hall,N.Shazeer,A.Kulshreshtha,H.-T.Cheng,A.Jin,T.Bos,
L. Baker, Y. Du, et al. Lamda: Language models for dialog applications. arXiv preprint
arXiv:2201.08239,2022.
[57] H.Touvron,T.Lavril,G.Izacard,X.Martinet,M.-A.Lachaux,T.Lacroix,B.Rozière,N.Goyal,
E.Hambro, F.Azhar, etal. Llama: Openandefficientfoundationlanguagemodels. arXiv
preprintarXiv:2302.13971,2023.
[58] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman. Glue: A multi-
task benchmark and analysis platform for natural language understanding. arXiv preprint
arXiv:1804.07461,2018.
[59] Y.Wang,Y.Kordi,S.Mishra,A.Liu,N.A.Smith,D.Khashabi,andH.Hajishirzi. Self-instruct:
Aligninglanguagemodelwithselfgeneratedinstructions. arXivpreprintarXiv:2212.10560,
2022.
[60] Y.Wang,S.Mishra,P.Alipoormolabashi,Y.Kordi,A.Mirzaei,A.Arunkumar,A.Ashok,A.S.
Dhanasekaran,A.Naik,D.Stap,etal. Super-naturalinstructions:generalizationviadeclarative
instructionson1600+tasks. InEMNLP,2022.
[61] Y. Wang, S. Mishra, P. Alipoormolabashi, Y. Kordi, A. Mirzaei, A. Naik, A. Ashok, A. S.
Dhanasekaran, A.Arunkumar, D.Stap, etal. Super-naturalinstructions: Generalizationvia
declarativeinstructionson1600+nlptasks.InProceedingsofthe2022ConferenceonEmpirical
MethodsinNaturalLanguageProcessing,pages5085–5109,2022.
[62] J.Wei,M.Bosma,V.Y.Zhao,K.Guu,A.W.Yu,B.Lester,N.Du,A.M.Dai,andQ.V.Le.
Finetunedlanguagemodelsarezero-shotlearners. arXivpreprintarXiv:2109.01652,2021.
[63] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. H. Chi, Q. V. Le, D. Zhou, et al.
Chain-of-thoughtpromptingelicitsreasoninginlargelanguagemodels. InAdvancesinNeural
InformationProcessingSystems,2022.
[64] T.Wolf,L.Debut,V.Sanh,J.Chaumond,C.Delangue,A.Moi,P.Cistac,T.Rault,R.Louf,
M.Funtowicz,etal. Huggingface’stransformers: State-of-the-artnaturallanguageprocessing.
arXivpreprintarXiv:1910.03771,2019.
[65] M.Wortsman,T.Dettmers,L.Zettlemoyer,A.Morcos,A.Farhadi,andL.Schmidt. Stableand
low-precisiontrainingforlarge-scalevision-languagemodels.arXivpreprintarXiv:2304.13013,
2023.
[66] G.Xiao, J.Lin, M.Seznec, J.Demouth, andS.Han. Smoothquant: Accurateandefficient
post-trainingquantizationforlargelanguagemodels. arXivpreprintarXiv:2211.10438,2022.
[67] T.Xie, C.H.Wu, P.Shi, R.Zhong, T.Scholak, M.Yasunaga, C.-S.Wu, M.Zhong, P.Yin,
S.I.Wang,etal. Unifiedskg: Unifyingandmulti-taskingstructuredknowledgegroundingwith
text-to-textlanguagemodels. arXivpreprintarXiv:2201.05966,2022.
[68] Z.Yang,P.Qi,S.Zhang,Y.Bengio,W.Cohen,R.Salakhutdinov,andC.D.Manning. Hotpotqa:
Adatasetfordiverse,explainablemulti-hopquestionanswering. InProceedingsofthe2018
ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages2369–2380,2018.
[69] Z.Yao,R.Y.Aminabadi,M.Zhang,X.Wu,C.Li,andY.He.Zeroquant:Efficientandaffordable
post-trainingquantizationforlarge-scaletransformers. arXivpreprintarXiv:2206.01861,2022.
[70] E.B.Zaken,S.Ravfogel,andY.Goldberg. Bitfit: Simpleparameter-efficientfine-tuningfor
transformer-basedmaskedlanguage-models. arXivpreprintarXiv:2106.10199,2021.
[71] A.Zeng,X.Liu,Z.Du,Z.Wang,H.Lai,M.Ding,Z.Yang,Y.Xu,W.Zheng,X.Xia,etal.
Glm-130b: Anopenbilingualpre-trainedmodel. arXivpreprintarXiv:2210.02414,2022.
20
--- PAGE 21 ---
[72] S.Zhang,S.Roller,N.Goyal,M.Artetxe,M.Chen,S.Chen,C.Dewan,M.Diab,X.Li,X.V.
Lin,etal.Opt:Openpre-trainedtransformerlanguagemodels.arXivpreprintarXiv:2205.01068,
2022.
[73] R.Zhong,K.Lee,Z.Zhang,andD.Klein. Adaptinglanguagemodelsforzero-shotlearningby
meta-tuningondatasetandpromptcollections. arXivpreprintarXiv:2104.04670,2021.
21
--- PAGE 22 ---
A QLoRAvsStandardFinetuningExperimentalSetupDetails
A.1 HyperparametersforQLORA
WedoahyperparametersearchforLoRAoverthefollowingvariables: LoRAdropout{0.0,0.05,
0.1},LoRAr{8,16,32,64,128,256},LoRAlayers{key+query,allattentionlayers,allFFNlayers,
alllayers,attention+FFNoutputlayers}. WekeepLoRAαfixedandsearchthelearningrate,since
LoRAαisalwaysproportionaltothelearningrate.
WefindthatLoRAdropout0.05isusefulforsmallmodels(7B,13B),butnotforlargermodels(33B,
65B).WefindLoRArisunrelatedtofinalperformanceifLoRAisusedonalllayersascanbeseen
inFigure4
65.0
64.8
64.6
64.4
64.2
64.0
8 16 32 64
LoRA r
LeguoR
bits
4
Figure 4: LoRA r for LLaMA 7B models finetuned on Alpaca. Each dot represents a combination of
hyperparametersandforeachLoRAr werun3randomseedwitheachhyperparametercombination. The
performanceofspecificLoRArvaluesappearstobeindependentofotherhyperparameters.
A.2 Super-NaturalInstructionsExperimentalSetupDetails
WeusethesamepreprocessingoftheSuper-NaturalInstructiondatasetasWangetal.[60]. However,
wesplitthetrainingdataintrainingandvalidationdatasetsallowingustoperformmorerigorous
hyperparametertuningandearlystopping. Weusethesamehyperparametersdescribedinthepaper
fortrainingthevariousT5modelsizesontheSuper-NaturalInstructiondata. WeuseLoRAr =16
forsmall,medium,andlargeT5modelsandLoRAr =64forT5xlandxxlmodels. Wealsouse
LoRAα=64inallourexperimentsandnoLoRAdropout.
B TrainingaState-of-the-artChatbotExperimentalSetupDetails
B.1 Datasets
WedescribethedatasetsusedforQLORAfinetuningexperimentsoutlinedinSection5.
OASST1 TheOpenAssistantdataset[31]wascollectedviacrowd-sourcing. Itcontains161,443
uniquemessagesdistributedacross66,497conversationsandspanning35differentlanguages. The
datasetoftencontainsseveralrankedrepliesforeachgivenuserquestion. Inourexperiments,we
onlyusethetopreplyateachlevelintheconversationtree. Thislimitsthedatasetto9,209examples.
Wefinetuningourmodelsonthefullconversationincludingtheuserqueries.
HH-RLHF Thisisahumanpreferencedatasetabouthelpfulnessandharmlessness. Eachdatapoint
consistsoftwoassistantrepliestoauserquestionalongwithahumanpreferencejudgmentofthe
bestreply. Thedatasetcontains160,800examples. Whenfinetuningonthisdataset,wecombine
helpfulnessandharmlessnessdataandonlykeepthepreferredassistantreply.
FLANv2 TheFLANv2collection[39]isacollectionof1836tasksaugmentedwithhundreds
ofmanuallycuratedtemplatesandrichformattingpatternsintoover15Mexamples. Theauthors
showthatmodelstrainedonthiscollectionoutperformotherpubliccollectionsincludingtheoriginal
FLAN 2021 [62], T0++ [50], Super-Natural Instructions [60], and OPT-IML [29]. We used the
sametaskmixturesdescribedbytheauthorswiththeexceptionofsomedatasetsthatwerenotfreely
availableatthetimeofwriting.
22
--- PAGE 23 ---
Parameters Dataset Batchsize LR Steps SourceLength TargetLength
7B All 16 2e-4 10000 384 128
7B OASST1 16 2e-4 1875 - 512
7B HH-RLHF 16 2e-4 10000 - 768
7B Longform 16 2e-4 4000 512 1024
13B All 16 2e-4 10000 384 128
13B OASST1 16 2e-4 1875 - 512
13B HH-RLHF 16 2e-4 10000 - 768
13B Longform 16 2e-4 4000 512 1024
33B All 32 1e-4 5000 384 128
33B OASST1 16 1e-4 1875 - 512
33B HH-RLHF 32 1e-4 5000 - 768
33B Longform 32 1e-4 2343 512 1024
65B All 64 1e-4 2500 384 128
65B OASST1 16 1e-4 1875 - 512
65B HH-RLHF 64 1e-4 2500 - 768
65B Longform 32 1e-4 2343 512 1024
Table9:TraininghyperparametersforQLORAfinetuningondifferentdatasetsandacrossmodelsizes.
Self-Instruct,Alpaca,UnnaturalInstructions TheSelf-Instruct,Alpaca,andUnnaturalInstruc-
tionsdatasets[59,55,26]areinstructiontuningdatasetscollectedwithvariousapproachesofmodel
distillation from GPT-3 Instruct and ChatGPT. They rely on prompting, in-context learning, and
paraphrasing to come up with diverse sets of instructions and outputs. The datasets comprise of
82,612,51,942,and240,670examplesrespectively. Oneadvantageofsuchdistilleddatasetsisthat
theycontainamorediversesetofinstructionstylescomparedtotheFLANv2collectionandsimilar
instructiontuningcollections.
Longform TheLongFormdataset[30]isbasedonanEnglishcorpusaugmentedwithinstructions
andassuchisahybridhuman-generateddataset. Theunderlyingdocumentsarehuman-writtenand
comefromC4andWikipediawhiletheinstructionsaregeneratedvisaLLMs. Thedatasetisextended
withadditionalstructuredcorporaexamplessuchasStackExchangeandWikiHowandtaskexamples
suchasquestionanswering,emailwriting,grammarerrorcorrection,story/poemgeneration,andtext
summarization. Thedatasetcontains23,700examples.
Chip2 ispartoftheOIGLaiondataset. ItcontainsPythoncodeexamples,naturalinstructionexam-
ples,genericharmlessinstructions,instruction/responseswithlists,follow-upquestions,Wikipedia
toxic adversarial questions, grade school math, reasoning instructions, and character and scene
descriptionswithatotalof210,289examples.
B.2 Hyperparameters
WeprovidetheexacthyperparametersusedinourQLORAfinetuningexperiments. Wefindhyper-
parameterstobelargelyrobustacrossdatasets. WeusetheMMLU5-shotdevsetforvalidation
andhyperparametertuning. InallourexperimentsweuseNF4withdoublequantizationandbf16
computationdatatype. WesetLoRAr =64,α=16,andaddLoRAmodulesonalllinearlayersof
thebasemodel. WealsouseAdambeta2of0.999,maxgradnormof0.3andLoRAdropoutof0.1
formodelsupto13Band0.05for33Band65Bmodels. Followingpreviousworkoninstruction
finetuning [62, 60] and after benchmarking other linear and cosine schedules, we use a constant
learningrateschedule. Weusegroup-by-lengthtogroupexamplesofsimilarlengthsinthesame
batch(notethiswillproduceaoscillatinglosscurve). Thehyperparameterswetuneforeachmodel
sizeareshowninTable9.
B.3 Ablations
Whileitisgeneralpracticeintheliteraturetoonlytrainontheresponseininstructionfollowing
datasets,westudytheeffectoftrainingontheinstructioninadditiontotheresponseinTable10. In
theseexperiments,werestrictthetrainingdatato52,000examplesandusethe7Bmodel. Overfour
differentinstructiontuningdatasets,wefindthatonlytrainingonthetargetisbeneficialtoMMLU
23
--- PAGE 24 ---
Dataset UnnaturalInstructions Chip2 Alpaca FLANv2 Mean
Trainonsourceandtarget 36.2 33.7 38.1 42.0 37.5
Trainontarget 38.0 34.5 39.0 42.9 38.6
Table10:MMLU5-shottestresultsstudyingtheeffectoftrainingontheinstructionsinadditiontotheresponse.
performance. Wedidnotevaluatetheeffectthismayhaveonchatabotperformanceasmeasuredby
vicunaorOAbenchmarks.
B.4 Whatismoreimportant: instructionfinetuningdatasetsizeordatasetquality?
Datasetsuitabilityismoreimportantthandatasetsize. Tounderstandtheeffectsofdataset
qualityvs. datasetsize,weexperimentwithsubsamplinglargedatasetswithatleast150,000samples
(Chip2,FLANv2,UnnaturalInstructions),intodatasetsofsize50,000,100,000and150,000and
examine the resulting trends, as shown in Table 11. We find that increasing the dataset size and
increasingthenumberofepochsimprovesMMLUonlymarginally(0.0-0.5MMLU),whilethe
differencebetweendatasetsisupto40xlarger(1.5-8.0MMLU).Thisisaclearindicatorthatdataset
qualityratherthandatasetsizeiscriticalformeanMMLUaccuracy. Weobtainsimilarfindingsfor
chatbotperformanceasdiscussedin.
C HumanEvaluation
We conduct a human evaluation with the same wording given to GPT-4 in the original Vicuna
evaluation[10],adjustedforanAmazonMechanicalTurkformasshowinFigure5.
D PairwiseEvaluationwithGPT-4
While we found that the GPT-4 evaluation gave different results depend on which system was
presented first, when averaged over both options the pairwise results were well-ordered. The
aggregatedpairwisejudgmentsarehowninTable12. Oninspection,itisclearthesejudgmentsare
transitive,i.e.,whenSystemAisjudgedbetterthanSystemBandSystemBisjudgedbetterthan
SystemC,itisalwaysthecasethatSystemAisjudgedbetterthanSystemC.Thisyieldsacomplete
ordering,giveninTable13.
E NormalFloat4-bitdatatype
TheexactvaluesoftheNF4datatypeareasfollows:
[-1.0,-0.6961928009986877,-0.5250730514526367,
-0.39491748809814453,-0.28444138169288635,-0.18477343022823334,
-0.09105003625154495,0.0,0.07958029955625534,0.16093020141124725,
0.24611230194568634,0.33791524171829224,0.44070982933044434,
0.5626170039176941,0.7229568362236023,1.0]
F NormalityofTrainedNeuralNetworkWeights
Whileitiscommonknowledgethattrainedneuralnetworkweightsaremostlynormallydistributed,
weperformstatisticaltestingtoverifythis. WeusetheShapiro-Wilktest[53]ontheweightsofthe7B
Table11:Effectdifferentdatasetsizesandfinetuningepochsonmean5-shotMMLUtestsetaccuracy.While
increasingthedatasetsizeandtrainingformorethan1epochshelpswithMMLUperformance,thedifference
betweendatasetsarefarlarger,indicatingthatdatasetqualityaffectsMMLUperformancemorethandatasetsize.
Chip UnnaturalInstructions FLANv2
Datapoints↓Epochs→ 1 2 3 1 2 3 1 2 3 Mean
50000 34.50 35.30 34.70 38.10 42.20 38.10 43.00 43.50 44.10 39.28
100000 33.70 33.90 34.00 40.10 41.20 37.00 43.90 43.70 44.90 39.16
150000 34.40 34.80 35.10 39.70 41.10 41.50 44.60 45.50 43.50 40.02
Mean 34.20 34.67 34.60 39.30 41.50 38.87 43.83 44.23 44.17
24
--- PAGE 25 ---
Figure5:Thecrowdsourcingformusedbyhumanannotators.
LLaMAmodel[57]. Wefindthattheweightsofeachhiddenunithavedifferentnormaldistributions.
Assuch,wetestheweightsofeachindividualhiddenunit. ThismeanforweightW ∈ Rin×out
weperformtestsovertheoutdimension. Usinga5%significancethreshold,wefindthat7.5%of
neurons are non-normally distributed which is about 2.5% more than the expected false-positive
rate. Assuch,whilealmostallpretrainedweightsappeartobenormallydistributedthereseemto
beexceptions. Suchexceptionsmightbeduetooutliersweights[13]orbecausethep-valueofthe
Shaprio-Wilktestisnotaccurateforlargesamplessizes[53]thatoccurintheLLaMAFFNlayer
hiddenunits. thisverifiestheclaimthatneuralnetworkweights.
Table12:AggregatedpairwiseGPT-4judgmentsbetweensystemswherethevalueofacellatrowxandcolumn
yis #judgmentxisbetterthany−#judgmentyisbetterthanx
total#numberofjudgments
Model Guanaco65B Guanaco33B Vicuna ChatGPT-3.5Turbo Bard Guanaco13B Guanaco7B
Guanaco65B - 0.21 0.19 0.16 0.72 0.59 0.86
Guanaco33B -0.21 - 0.17 0.10 0.51 0.41 0.68
Vicuna -0.19 -0.17 - 0.10 0.50 0.20 0.57
ChatGPT-3.5Turbo -0.16 -0.10 -0.10 - 0.35 0.19 0.40
Bard -0.72 -0.51 -0.50 -0.35 - 0.12 0.03
Guanaco13B -0.59 -0.41 -0.20 -0.19 -0.12 - 0.20
Guanaco7B -0.86 -0.68 -0.57 -0.40 -0.03 -0.20 -
25
--- PAGE 26 ---
Input gradient Optimizer Weight gradient Adapters Model
100%
75%
50%
25%
0%
7B (6.9 GB) 13B (11.3 GB) 33B (24.7 GB) 65B (45.0 GB)
LLaMA model size
Figure6:BreakdownofthememoryfootprintofdifferentLLaMAmodels.Theinputgradientsizeisforbatch
size1andsequencelength512andisestimatedonlyforadaptersandthebasemodelweights(noattention).
NumbersonthebarsarememoryfootprintinMBofindividualelementsofthetotalfootprint. Whilesome
modelsdonotquitefitoncertainGPUs,pagedoptimzierprovideenoughmemorytomakethesemodelsfit.
G MemoryFootprint
The memory footpring for QLoRA training with different LLaMA base models can be seen in
Figure 6. We see that the 33B model does not quite fit into a 24 GB and that paged optimizers
are needed to train it. Depicted is also batch size 1 with a sequence length of 512 and gradient
checkpointning. Thismeans,ifoneusesalargerbatchsize,orifalongsequenceisprocessed,the
activationgradientmightconsumeaconsiderableamountofmemory.
Table13:ThecompleteorderinginducedbypairwiseGPT-4judgmentsbetweensystems
Model Params Size
Guanaco 65B 41GB
Guanaco 33B 21GB
Vicuna 13B 26GB
ChatGPT-3.5Turbo N/A N/A
Bard N/A N/A
Guanaco 13B 10GB
Guanaco 7B 5GB
26